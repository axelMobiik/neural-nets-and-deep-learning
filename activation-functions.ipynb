{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1580006a-181d-4856-8090-3e0b5f07999f",
   "metadata": {},
   "source": [
    "<h1>Activation Functions</h1>\n",
    "<p>\n",
    "    Recall that inputs <strong>x</strong> have a weight <strong>w</strong> and a bias term <strong>b</strong> attached to them in the perceptron model.\n",
    "    <br>Wich means we have \n",
    "    <li><strong>x*w+b</strong></li>\n",
    "    Clearly <strong>w</strong> implies hoe much weight or strength to give the incoming input.\n",
    "    <br>We can think of <strong>b</strong> as an offset value, making <strong>x*w</strong> have to reach a certain threshold before having an effect.\n",
    "    <br>For example if <strong>b = -10</strong>\n",
    "    <li><strong>x*w + b</strong></li>\n",
    "    Then the offects of <strong>x*w</strong> won't really start to overcome the bias until their product surpasses 10.\n",
    "    <br>After that, then the affect is solely based on the value of <strong>w</strong>.\n",
    "    <br>Thus the term \"bias\".\n",
    "</p>\n",
    "<p>\n",
    "    Next we want to set boundaries for the overall output value of:\n",
    "    <li><strong>x*w + b</strong></li>\n",
    "    We can state:\n",
    "    <li><strong>z = x*w + b</strong></li>\n",
    "    And then pass <strong>z</strong> through some activation function to limit its value.\n",
    "</p>\n",
    "<p>\n",
    "    A lot of research has been done into activation functions and their effectiveness.\n",
    "    <br>Let's explore some common activation functions.\n",
    "    <br>Recall our simple perceptron has an f(x). If we had a binary classification problem, we would want an output of either 0 or 1.\n",
    "    <br>To avoid confusion, let's define the total inputs as a variable <strong>z</strong>.\n",
    "    <br>Where <strong>z = wx + b</strong>\n",
    "    <br>In this context, we'll then refer to activation functions as <strong>f(z)</strong>.\n",
    "    <br>Keep in mind, you will often see these variables capitalized <strong>f(z)</strong> or <strong>X</strong> to denote a tensor input consisting of multiple values.\n",
    "</p>\n",
    "<p>\n",
    "    The most simple networks rely on a basic <strong>step function</strong> that outputs 0 or 1.\n",
    "    <br>This sort of function could be useful for classification (0 or 1 class).\n",
    "    <br>However this is a very \"strong\" function, since small changes aren't reflected.\n",
    "    <br>There is just an immediate cut of that splits between 0 and 1.\n",
    "    <br>It would be nice if we could have a more dynamic function, lucky for us, this is the sigmoid function (f(z) = 1 / (1 + e^(-z))).\n",
    "    <br>Changing the activation function used can be beneficial depending on the task!\n",
    "    <br>This still works for classification, and will be more sensitive to small changes\n",
    "</p>\n",
    "<p>\n",
    "    Let's discuss a few more activation functions that we'll encounter!\n",
    "    <li>Hyperbolic Tangent: tanh(z)</li>\n",
    "    <li>> Outputs between -1 and 1 instead of 0 to 1</li>\n",
    "    <li>Rectified Linear Unit (ReLU):</li>\n",
    "    <li>> This is actually a relatively simple function: max(0, z)</li>\n",
    "    <li>> ReLu has been found to have very good performance, especially when dealing with the issue of <strong>vanish gradient</strong>. We'll often default to ReLu due to its overall good performance.</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667628e3-2cdc-410e-b8fe-63d53cca8efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
