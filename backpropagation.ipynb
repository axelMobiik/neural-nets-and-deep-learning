{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8c48a9-386d-4c2f-b8d3-852614d21268",
   "metadata": {},
   "source": [
    "<h1>Backpropagation</h1>\n",
    "<p>\n",
    "    The last theory topic we will cover is backpropagation\n",
    "    <br>We'll start by building an intuition behind backpropagation, and then we'll dive into the calculus and notation of backpropagation\n",
    "</p>\n",
    "<p>\n",
    "    Fundamentally, we want to know how the cost function results changes with respect to the weights in the network, so we can update the weights to minimize the cost function\n",
    "    <br>Let's begin with a very simple network, where each layer only has 1 neuron\n",
    "    <br>Each input will receive a weight and bias. This means we have: C(w1,b1,w2,b2,w3,b3)\n",
    "    <br>We've already seen how this process propagates forward.\n",
    "    <br>Let's start the end to see the backpropagation\n",
    "    <br>Let's say we have L layers. Focusing on these last two layers, let's define x=wx+b\n",
    "    <br>Then applying an activation function we'll state: a=o(z)\n",
    "    <br>This means we have: z^L = w^L * a^(L-1) * b^L\n",
    "    <br>a^L = o(z^L)\n",
    "    <br>C0(...) = (a^L - y)^2\n",
    "    <br>We want to understand how sensitive is the cost function to changes in W\n",
    "    <br>Using the relationships we already know along with the chain rule\n",
    "    <br>We can calculate the same for the bias terms\n",
    "</p>\n",
    "<p>\n",
    "    The main idea there is that we can use the gradient to go back through the network and adjust our weights and biases to minimize the output of the error vector on the last output layer.\n",
    "    <br>Using some calculus notation, we can expand this idea to networks with multiple neurons per layer. Hadamard Product.\n",
    "</p>\n",
    "<p>\n",
    "    Given this notation and backpropafation, we have a few main steps to training neural networks. Note! you do not need to fully understand these intricate details to continue with the coding portions.\n",
    "    <br>Step 1: Using input x set the activation function a for the input layer\n",
    "    <li>z = wx + b</li>\n",
    "    <li>a = o(z)</li>\n",
    "    This resulting a then feeds into the next layer (and so on).\n",
    "    <br>\n",
    "    <br>Step 2: For each layer, compute\n",
    "    <br>z^L = w^L * a^L-1 + b^L\n",
    "    <br>a^L = o(z^L)\n",
    "    <br>\n",
    "    <br>Step 3: We compute our error vector\n",
    "    <br>Noew let's write out our error term for a layer in terms of the error of the next layer (since we're moving backwards).\n",
    "    <br>Font Note: lowecase L\n",
    "    <br>Font Note: Number 1\n",
    "    <br>\n",
    "    <br>Step 4: Backpropagate the error\n",
    "    <br>When we apply the transpose weight matrix, we can think intuitively of this as moving the error backward through the network, giving us some sort of measure of the error at the output of the lth layer\n",
    "    <br>We then take the Hadamard product. This moves the error backward through the activation function in layer l, giving us the error in the weighted input to layer l.\n",
    "    <br>This then allowes us to adjust the weights and biases to help minimize that cost function.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3c587-0160-4bcb-afd8-f5a82443e261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
