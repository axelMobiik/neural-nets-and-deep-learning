{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a785805e-d2f8-470c-9e8e-4f18ad1bc396",
   "metadata": {},
   "source": [
    "<h1>Neural Networks</h1>\n",
    "<p>\n",
    "    A single perceptron won't be enough to learn complicated systems.\n",
    "    <br>Fortunately, we can expand on the idea of a simgle perceptron, to create a multi-layer perceptron model.\n",
    "    <br>We'll also introduce the idea of activation functions.\n",
    "    <br>To build a network of perceptrons, we can connect layers of perceptrons, using a <strong>multi-layer perceptron model</strong>\n",
    "    <br>The outputs of one perceptron are directly fed into as inputs to another perceptron.\n",
    "    <br>This allows the network as a whole to learn about interactions and relationships between features.\n",
    "    <li>The first layer is the <strong>input layer</strong></li>\n",
    "    <li>The last layer is the <strong>output layer</strong>. Note: This layer can be more than one neuron</li>\n",
    "    <li>Layers in between the input and output layers are the <strong>hidden layers</strong></li>\n",
    "    <li>> Hidden layers are difficult to interpret, due to their high interconnectivity and distance away from known input or output values.</li>\n",
    "    <li>> Neural Networks become <strong>\"deep neural networks\"</strong> if then contain 2 or more hidden layers</li>\n",
    "</p>\n",
    "<p>\n",
    "    Terminology:\n",
    "    <li>Input Layer: First layer that directly accepts real data values</li>\n",
    "    <li>Hidden Layer: Any layer between input and output layers</li>\n",
    "    <li>Output Layer: The final estimate of the output</li>\n",
    "</p>\n",
    "<p>\n",
    "    What is incredible about the neural network framework is that it can be used to approximate any function.\n",
    "    <br>Zhou Lu and later con Boris Hanin proved mathematically that Neural Network can approximate any convex continuous function.\n",
    "</p>\n",
    "<p>\n",
    "    Previously in our simple model we saw that the perceptron itself contained a very simple summation function f(x).\n",
    "    <br>For most use cases however that won't be useful, we'll want to be able to set constraints to our output values, especially in classification tasks.\n",
    "    <br>In a classification tasks, it would be useful to have all outputs fall between 0 and 1.\n",
    "    <br>These values can then present probability assignments for each class.\n",
    "    <br>In the next lecture, we'll explore how to use <strong>activation functions</strong> to set boundaries to output values from the neuron.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
