{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b639a74b-8002-4a6c-a404-1dc0b1dd207c",
   "metadata": {},
   "source": [
    "<h1>Cost Functions and Gradient Descent</h1>\n",
    "<p>\n",
    "    We now understand that neural networks take in inputs, multiply them by weights, and add biases to them.\n",
    "    <br>Then this result is passed through an activation function which at the end of all the layers leads to some output.\n",
    "    <br>This output <strong>y</strong> is the model's estimation of what it predicts the label to be.\n",
    "    <br>So after the network creates its prediction, how do we evaluate it?\n",
    "    <br>And after the evaluation how can we update the network's weights and biases?\n",
    "</p>\n",
    "<p>\n",
    "    We need to take the estimated outputs of the network and then compare them to the real values of the label.\n",
    "    <br>Keep in mind this is using the training data set during the fitting/training of the model.\n",
    "    <br>The cost function (often referred to as a loss function) must be an verage so it can output a single value.\n",
    "    <br>We can keep track of our loss/cost during training to monitor network performance.\n",
    "</p>\n",
    "<p>\n",
    "    We'll use the following variables:\n",
    "    <br>y to represent the true value\n",
    "    <br>a to represent neuron's prediction\n",
    "    <br>\n",
    "    <br>In terms of wights and bias:\n",
    "    <br>w*x + b = z\n",
    "    <br>Pass z into activation function o(z) = a\n",
    "</p>\n",
    "<p>\n",
    "    One very common cost function is the quadratic cost function\n",
    "    <br>We simply calculate the difference between the real values y(x) against our predicted values a(x)\n",
    "</p>\n",
    "<p>\n",
    "    We can think of the cost function as:\n",
    "    <br>C(W, B, S^r, E^r)\n",
    "    <br>W is our neural network's weights, B is our neural network's biases, S^r is the input of a single training sample, and E^r is the desired output of that training sample.\n",
    "    <br>Notice how that information was all encoded in our simplified notation.\n",
    "    <br>The a(x) holds information about weights and biases.\n",
    "    <br>This means that if we have a huge network, we can expect C to be quite complex, with hue vectors of weights and biases.\n",
    "    <br>That is a lot to calculate! how do we solve this?\n",
    "    <br>In a real case, this means we have some cost functions C dependent lots of weights!\n",
    "    <li>C(w1,w2,w3,...,wn)</li>\n",
    "    How do we figure out which weights lead us to the lowest cost?\n",
    "    <br>For simplicity, let's imagine we only had one weight in our cost function w.\n",
    "    <br>We want to minimize our loss/cost (overall error).\n",
    "    <br>Which means we need to figure out what value of w results in the minimum of C(w)\n",
    "</p>\n",
    "<p>\n",
    "    The learning rate we shoed in our illustration was constant (each step size was equal)\n",
    "    <br>But we can be clever and adapt our step size as we go along\n",
    "    <br>We could start with larger steps, then go smaller as we realize the slope gets closer to zero.\n",
    "    <br>This is knoen as adaptive gradient descent\n",
    "</p>\n",
    "<p>\n",
    "    In 2015, Kingma and Ba published their paper: \"Adam: A method for Stochastic Optimization\".\n",
    "    <br>Adam is a much more efficient way of searching for these minimums, so you will see us use it for our code!\n",
    "    <br>When dealing with these N-dimensional vectors (tensors), the notation changes from derivative to gradient.\n",
    "    <br>This means we calculate C(w1,w2,...,wn)\n",
    "</p>\n",
    "<p>\n",
    "    For classsification problems, we often use the cross entropy loss function.\n",
    "    <br>The assumption is that your model predicts a probability distribution p(y=i) for each class i=1,2,...,C\n",
    "</p>\n",
    "<p>\n",
    "    So far we understand how networks can take in input, effect that input with weights, biases and activation functions to produce an estimated output.\n",
    "    <br>Then we learned how to evaluate that output\n",
    "    <br>The last thing we need to learn about theory is:\n",
    "    <br>Once we get our cost/loss value, how do we actually go back and adjust our weights and biases?\n",
    "    <br>This is backpropatation, and it is what we are going to conver next!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220258f-6add-4b66-9926-1ba9671eba84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
